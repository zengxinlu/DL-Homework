# 		   MNIST实验报告

### 实验背景

​	代码用 Python 编写，使用 Tensorflow 进行神经网络的构建与学习。实验使用控制变量法对参数给模型带来的影响进行定量分析，分别对不同的神经网络结构进行实验测试。



### 实验结果

#### 神经网络结构一

​	输入层——隐藏层——输出层，使用 relu 作为激活函数，使用交叉熵作为损失函数，使用梯度下降法进行训练

##### 基础参数设置

1. 学习速率：0.01
2. 迭代次数：5000
3. 隐藏层节点数：300
4. 输入数据切分规模：100

- 学习速率对比实验

  | 学习速率 | 0.1    | 0.01   | 0.001  |
  | ---- | ------ | ------ | ------ |
  | 正确率  | 97.09% | 92.08% | 84.59% |

- 迭代次数对比实验

  | 迭代次数 | 2000   | 5000   | 10000  |
  | ---- | ------ | ------ | ------ |
  | 正确率  | 90.22% | 92.08% | 94.02% |

- 隐藏层节点数对比实验

  | 隐藏层节点数 | 128    | 300    | 1280   |
  | :----- | ------ | ------ | ------ |
  | 正确率    | 92.05% | 92.08% | 92.62% |

- 数据切分规模对比实验

  | 切分规模 | 50     | 100    | 200    |
  | ---- | ------ | ------ | ------ |
  | 正确率  | 92.20% | 92.08% | 92.16% |


##### 结论一

1. 学习速率对正确率影响较大，学习速率越快，正确率越高；

2. 迭代次数对正确率有一定影响，迭代次数越多，正确率越高，但是模型容易产生过拟合；

3. 隐藏节点规模和数据切分规模对正确率影响不大。

   ​

#### 神经网络结构二

​	输入层——隐藏层 1——隐藏层 2——输出层，使用 relu 作为激活函数，使用交叉熵作为损失函数，使用梯度下降法进行训练

##### 基础参数设置

1. 学习速率：0.01
2. 迭代次数：5000
3. 隐藏层 1 节点数：300
4. 隐藏层 2 节点数：150
5. 输入数据切分规模：100

- 隐藏层 2 节点数对比实验

  | 隐藏层2节点数 | 50     | 150    | 300    |
  | :------ | ------ | ------ | ------ |
  | 正确率     | 92.81% | 93.04% | 93.20% |


##### 结论二

1. 隐藏层由一层变为两层时，对正确率影响不大；
2. 隐藏层2节点数变化时，对正确率影响不大。